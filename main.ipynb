{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf87da32",
   "metadata": {},
   "source": [
    "With MySQL db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c9c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import regexp_replace, lower, col, from_json, udf\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from textblob import TextBlob\n",
    "import mysql.connector\n",
    "\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"SentimentAnalysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,mysql:mysql-connector-java:8.0.32\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Read data from Kafka topic\n",
    "kafka_brokers = \"localhost:9092\"\n",
    "topic_name = \"nyt\"\n",
    "\n",
    "kafka_data = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_brokers).option(\"subscribe\", topic_name).load()\n",
    "\n",
    "# Define the schema for your JSON data\n",
    "json_schema = StructType([StructField(\"headline\", StringType(), True),\n",
    "                         StructField(\"content\", StringType(), True),\n",
    "                         StructField(\"news_desk\", StringType(), True),\n",
    "                         StructField(\"section\", StringType(), True),\n",
    "                         StructField(\"source\", StringType(), True),\n",
    "                         StructField(\"word_count\", IntegerType(), True)])\n",
    "\n",
    "# Deserialize JSON data\n",
    "deserialized_data = kafka_data.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .withColumn(\"json_data\", from_json(\"value\", json_schema)).select(\"json_data.*\")\n",
    "\n",
    "deserialized_data = deserialized_data.withColumnRenamed(\"source\", \"source_\") #to match the field name in the db\n",
    "\n",
    "# Clean text by removing non-alphanumeric characters and converting to lowercase, remove numbers\n",
    "cleaned_data = deserialized_data.withColumn(\"cleaned_content\", \n",
    "                        lower(regexp_replace(col(\"content\"), \"[^a-zA-Z\\\\s]\", \"\")))\n",
    "#remove punctuations\n",
    "cleaned_data = cleaned_data.withColumn(\"cleaned_content\", regexp_replace(col(\"cleaned_content\"), r'[^\\w\\s]', ''))\n",
    "\n",
    "# Tokenization (splitting text into words)\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_content\", outputCol=\"words\")\n",
    "\n",
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# Define a UDF to join the words back into a single string\n",
    "def join_text(words):\n",
    "    return \" \".join(words)\n",
    "\n",
    "join_udf = udf(join_text, StringType())\n",
    "\n",
    "# Create a pipeline for preprocessing\n",
    "preprocessing_pipeline = Pipeline(stages=[tokenizer, remover])\n",
    "preprocessing_model = preprocessing_pipeline.fit(cleaned_data)\n",
    "preprocessed_data = preprocessing_model.transform(cleaned_data)\n",
    "\n",
    "# Apply the sentiment analysis UDF to the preprocessed content\n",
    "preprocessed_data = preprocessed_data.withColumn(\"cleaned_content\", join_udf(col(\"filtered_words\")))\n",
    "\n",
    "# Define a user-defined function to calculate polarity\n",
    "def Polarity(content: str) -> float:\n",
    "    return TextBlob(content).sentiment.polarity\n",
    "\n",
    "polarity_udf = udf(Polarity, FloatType())\n",
    "\n",
    "# Define UDF for sentiment analysis\n",
    "def Sentiment(polarity_value: float) -> str:\n",
    "    if polarity_value < 0:\n",
    "        return \"Negative\"\n",
    "    elif polarity_value == 0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "    \n",
    "sentiment_udf = udf(Sentiment, StringType())\n",
    "\n",
    "# Apply the sentiment analysis UDF to the preprocessed content\n",
    "# Calculate sentiment\n",
    "sentiment_df = preprocessed_data.withColumn(\"Polarity\", polarity_udf(col(\"cleaned_content\")))\n",
    "sentiment_df = sentiment_df.withColumn(\"Sentiment\", sentiment_udf(col(\"polarity\")))\n",
    "sentiment_df = sentiment_df.select(\"headline\", \"content\", \"cleaned_content\", \"news_desk\", \"section\", \"source_\",\n",
    "                                  \"word_count\", \"Polarity\", \"Sentiment\")\n",
    "\n",
    "#loading the sentiment_df dataframe to mysql db\n",
    "mysql_host_name = \"localhost\"\n",
    "mysql_port_no = \"3306\"\n",
    "mysql_database_name = \"nyt_demo\"\n",
    "mysql_driver_class= \"com.mysql.cj.jdbc.Driver\"\n",
    "mysql_table_name = \"articles\"\n",
    "mysql_user_name = \"root\"\n",
    "mysql_password = \"password\"\n",
    "mysql_jdbc_url = \"jdbc:mysql://\" + mysql_host_name + \":\" + mysql_port_no +\"/\" + mysql_database_name\n",
    "\n",
    "mysql_properties = {\n",
    "    \"url\": mysql_jdbc_url,\n",
    "    \"driver\": mysql_driver_class,\n",
    "    \"user\": mysql_user_name,\n",
    "    \"password\": mysql_password,\n",
    "}\n",
    "\n",
    "# Function to save the DataFrame to MySQL\n",
    "def save_to_mysql(df, properties, table_name):\n",
    "    df.write.jdbc(\n",
    "        url=properties[\"url\"],\n",
    "        table=table_name,\n",
    "        mode=\"append\",  \n",
    "        properties={\n",
    "            \"driver\": properties[\"driver\"],\n",
    "            \"user\": properties[\"user\"],\n",
    "            \"password\": properties[\"password\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Start the Spark stream to read from Kafka\n",
    "query = sentiment_df.writeStream.outputMode(\"append\").foreachBatch(\n",
    "    lambda df, epoch_id: save_to_mysql(df, mysql_properties, mysql_table_name)\n",
    ").start()\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
